---
title: Embedding models
description: Dense and sparse embedding models with FlashInfer acceleration and SGLang's batching infrastructure.
---

SGLang provides robust support for embedding models by integrating efficient serving mechanisms with its flexible programming interface. This integration allows for streamlined handling of embedding tasks, facilitating faster and more accurate retrieval and semantic search operations. SGLang's architecture enables better resource utilization and reduced latency in embedding model deployment.

<Warning>
  Embedding models must be launched with the `--is-embedding` flag. Some models
  may also require `--trust-remote-code`.
</Warning>

## Quick start

<Steps>
  <Step title="Launch the server">
    ```bash Command
    python3 -m sglang.launch_server \
      --model-path Qwen/Qwen3-Embedding-4B \
      --is-embedding \
      --host 0.0.0.0 \
      --port 30000
    ```
  </Step>

  <Step title="Send a client request">
    ```python Example
    import requests

    url = "http://127.0.0.1:30000"

    payload = {
        "model": "Qwen/Qwen3-Embedding-4B",
        "input": "What is the capital of France?",
        "encoding_format": "float"
    }

    response = requests.post(url + "/v1/embeddings", json=payload).json()
    print("Embedding:", response["data"][0]["embedding"])
    ```

  </Step>
</Steps>

## Multimodal embedding example

For multimodal models like GME that support both text and images:

<Steps>
  <Step title="Launch the server with a multimodal model">
    ```bash Command
    python3 -m sglang.launch_server \
      --model-path Alibaba-NLP/gme-Qwen2-VL-2B-Instruct \
      --is-embedding \
      --chat-template gme-qwen2-vl \
      --host 0.0.0.0 \
      --port 30000
    ```
  </Step>

  <Step title="Send a multimodal request">
    ```python Example
    import requests

    url = "http://127.0.0.1:30000"

    text_input = "Represent this image in embedding space."
    image_path = "https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild/resolve/main/images/023.jpg"

    payload = {
        "model": "gme-qwen2-vl",
        "input": [
            {"text": text_input},
            {"image": image_path}
        ],
    }

    response = requests.post(url + "/v1/embeddings", json=payload).json()
    print("Embeddings:", [x.get("embedding") for x in response.get("data", [])])
    ```

  </Step>
</Steps>

## Matryoshka embedding example

[Matryoshka Embeddings](https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html#matryoshka-embeddings) or [Matryoshka Representation Learning (MRL)](https://arxiv.org/abs/2205.13147) is a technique used in training embedding models. It allows users to trade off between performance and cost.

<Steps>
  <Step title="Launch a Matryoshka-capable model">
    If the model config already includes `matryoshka_dimensions` or `is_matryoshka` then no override is needed. Otherwise, use `--json-model-override-args` as below:

    ```bash Command
    python3 -m sglang.launch_server \
      --model-path Qwen/Qwen3-Embedding-0.6B \
      --is-embedding \
      --host 0.0.0.0 \
      --port 30000 \
      --json-model-override-args '{"matryoshka_dimensions": [128, 256, 512, 1024, 1536]}'
    ```

    <Info>
      Setting `"is_matryoshka": true` allows truncating to any dimension. Otherwise, the server validates that the specified dimension in the request is one of `matryoshka_dimensions`. Omitting `dimensions` in a request returns the full vector.
    </Info>

  </Step>

  <Step title="Make requests with different output dimensions">
    ```python Example
    import requests

    url = "http://127.0.0.1:30000"

    # Request a truncated (Matryoshka) embedding by specifying a supported dimension.
    payload = {
        "model": "Qwen/Qwen3-Embedding-0.6B",
        "input": "Explain diffusion models simply.",
        "dimensions": 512  # change to 128 / 1024 / omit for full size
    }

    response = requests.post(url + "/v1/embeddings", json=payload).json()
    print("Embedding:", response["data"][0]["embedding"])
    ```

  </Step>
</Steps>

## Supported models

<table style={{width: "100%", borderCollapse: "collapse", tableLayout: "fixed"}}>
  <colgroup>
    <col style={{width: "25%"}} />
    <col style={{width: "25%"}} />
    <col style={{width: "25%"}} />
    <col style={{width: "25%"}} />
  </colgroup>
  <thead>
    <tr style={{borderBottom: "2px solid #d55816"}}>
      <th style={{textAlign: "left", padding: "10px 12px", fontWeight: 700, whiteSpace: "nowrap", backgroundColor: "rgba(255,255,255,0.02)"}}>Model</th>
      <th style={{textAlign: "left", padding: "10px 12px", fontWeight: 700, whiteSpace: "nowrap", backgroundColor: "rgba(255,255,255,0.05)"}}>Example HF model</th>
      <th style={{textAlign: "left", padding: "10px 12px", fontWeight: 700, whiteSpace: "nowrap", backgroundColor: "rgba(255,255,255,0.02)"}}>Chat template</th>
      <th style={{textAlign: "left", padding: "10px 12px", fontWeight: 700, whiteSpace: "nowrap", backgroundColor: "rgba(255,255,255,0.05)"}}>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>E5 (Llama/Mistral based)</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`intfloat/e5-mistral-7b-instruct`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>N/A</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>High-quality text embeddings based on Mistral/Llama architectures</td>
    </tr>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>GTE-Qwen2</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`Alibaba-NLP/gte-Qwen2-7B-instruct`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>N/A</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>Alibaba's text embedding model with multilingual support</td>
    </tr>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>Qwen3-Embedding</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`Qwen/Qwen3-Embedding-4B`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>N/A</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>Latest Qwen3-based text embedding model for semantic representation</td>
    </tr>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>BGE</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`BAAI/bge-large-en-v1.5`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>N/A</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>BAAI's text embeddings (requires `--attention-backend triton` or `torch_native`)</td>
    </tr>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>GME (Multimodal)</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`Alibaba-NLP/gme-Qwen2-VL-2B-Instruct`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>`gme-qwen2-vl`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>Multimodal embedding for text and image cross-modal tasks</td>
    </tr>
    <tr>
      <td style={{padding: "9px 12px", fontWeight: 500, backgroundColor: "rgba(255,255,255,0.02)"}}>CLIP</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>`openai/clip-vit-large-patch14-336`</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.02)"}}>N/A</td>
      <td style={{padding: "9px 12px", backgroundColor: "rgba(255,255,255,0.05)"}}>OpenAI's CLIP for image and text embeddings</td>
    </tr>
  </tbody>
</table>
